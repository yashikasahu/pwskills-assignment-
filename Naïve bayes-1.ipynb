{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28580c2b",
   "metadata": {},
   "source": [
    "Great! Let's work through each question about **Bayes’ Theorem and Naive Bayes**, step-by-step — and then we’ll dive into that assignment example.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q1. What is Bayes' Theorem?**\n",
    "\n",
    "Bayes’ Theorem is a way to **update probabilities** based on new evidence. It answers the question:\n",
    "\n",
    "> \"Given what I already know, how should I update my belief in a hypothesis when I see new data?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. What is the formula for Bayes' Theorem?**\n",
    "\n",
    "\\[\n",
    "P(H | E) = \\frac{P(E | H) \\cdot P(H)}{P(E)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(H | E) \\): Posterior probability (probability of hypothesis \\( H \\) given evidence \\( E \\))\n",
    "- \\( P(E | H) \\): Likelihood (probability of evidence given the hypothesis)\n",
    "- \\( P(H) \\): Prior probability (belief in the hypothesis before seeing evidence)\n",
    "- \\( P(E) \\): Marginal probability (total probability of the evidence)\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. How is Bayes' Theorem used in practice?**\n",
    "\n",
    "It’s used in many areas, including:\n",
    "- **Spam detection** (Naive Bayes)\n",
    "- **Medical diagnosis**\n",
    "- **Machine learning classification**\n",
    "- **Recommendation systems**\n",
    "- **Risk prediction / fraud detection**\n",
    "\n",
    "It’s especially useful where we want to **predict probabilities** of events given prior data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What is the relationship between Bayes' Theorem and conditional probability?**\n",
    "\n",
    "Bayes’ Theorem is **derived from the definition of conditional probability**:\n",
    "\n",
    "\\[\n",
    "P(A | B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{and} \\quad P(B | A) = \\frac{P(B \\cap A)}{P(A)}\n",
    "\\]\n",
    "\n",
    "Rearranging these gives Bayes’ Rule:\n",
    "\\[\n",
    "P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "So, **Bayes’ Theorem is essentially a conditional probability flipped using the chain rule.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?**\n",
    "\n",
    "There are **three main types** of Naive Bayes classifiers:\n",
    "\n",
    "| Type             | Use case                                  | Feature types            |\n",
    "|------------------|--------------------------------------------|---------------------------|\n",
    "| **GaussianNB**   | Features are **continuous**               | Assumes normal distribution |\n",
    "| **MultinomialNB**| Features are **discrete counts** (e.g. words) | Text classification, bag-of-words |\n",
    "| **BernoulliNB**  | Features are **binary** (0 or 1)          | Binary features, presence/absence |\n",
    "\n",
    "✔️ **Choose based on your data type** (numeric → Gaussian, count → Multinomial, binary → Bernoulli).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. Assignment — Classifying using Naive Bayes**\n",
    "\n",
    "We are given frequencies for features \\( X_1 \\) and \\( X_2 \\) for classes A and B.\n",
    "\n",
    "#### ➤ New instance:\n",
    "- \\( X_1 = 3 \\), \\( X_2 = 4 \\)\n",
    "\n",
    "#### ➤ Frequency table:\n",
    "\n",
    "| Class | X1=3 | X2=4 | Total Samples |\n",
    "|-------|------|------|----------------|\n",
    "| A     | 4    | 3    | \\(3+3+4 = 10\\) for X1 and \\(4+3+3+3 = 13\\) for X2 |\n",
    "| B     | 1    | 3    | \\(2+2+1 = 5\\) for X1 and \\(2+2+2+3 = 9\\) for X2 |\n",
    "\n",
    "#### ➤ Step 1: Assume **equal priors**  \n",
    "\\[\n",
    "P(A) = P(B) = 0.5\n",
    "\\]\n",
    "\n",
    "#### ➤ Step 2: Compute likelihoods  \n",
    "We’ll use **likelihood estimates** based on frequency:\n",
    "\n",
    "\\[\n",
    "P(X_1 = 3 | A) = \\frac{4}{10}, \\quad P(X_2 = 4 | A) = \\frac{3}{13}\n",
    "\\]\n",
    "\\[\n",
    "P(X_1 = 3 | B) = \\frac{1}{5}, \\quad P(X_2 = 4 | B) = \\frac{3}{9}\n",
    "\\]\n",
    "\n",
    "#### ➤ Step 3: Compute posteriors (unnormalized)\n",
    "\n",
    "For class A:\n",
    "\\[\n",
    "P(A | X_1=3, X_2=4) \\propto P(X_1=3 | A) \\cdot P(X_2=4 | A) \\cdot P(A) = \\frac{4}{10} \\cdot \\frac{3}{13} \\cdot 0.5\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 0.4 \\cdot 0.2308 \\cdot 0.5 = 0.0462\n",
    "\\]\n",
    "\n",
    "For class B:\n",
    "\\[\n",
    "P(B | X_1=3, X_2=4) \\propto \\frac{1}{5} \\cdot \\frac{3}{9} \\cdot 0.5 = 0.2 \\cdot 0.333 \\cdot 0.5 = 0.0333\n",
    "\\]\n",
    "\n",
    "#### ➤ Step 4: Compare posteriors\n",
    "\n",
    "- \\( P(A | \\text{data}) \\approx 0.0462 \\)\n",
    "- \\( P(B | \\text{data}) \\approx 0.0333 \\)\n",
    "\n",
    "✔️ **Conclusion**: Class **A** is more likely.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see this coded in Python or with Laplace smoothing added (in case of zero frequencies)?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
