{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021c8233",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?**\n",
    "\n",
    "You're already given:\n",
    "- \\( P(\\text{Uses Plan}) = 0.70 \\)\n",
    "- \\( P(\\text{Smoker} | \\text{Uses Plan}) = 0.40 \\)\n",
    "\n",
    "You‚Äôre asked:  \n",
    "‚û°Ô∏è **What is \\( P(\\text{Smoker} | \\text{Uses Plan}) \\)?**\n",
    "\n",
    "But that's already given directly in the question:  \n",
    "‚úÖ **Answer: 0.40** (or 40%)\n",
    "\n",
    "No Bayes‚Äô Theorem needed here since it‚Äôs already conditional.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**\n",
    "\n",
    "| Feature | **Bernoulli Naive Bayes** | **Multinomial Naive Bayes** |\n",
    "|--------|----------------------------|------------------------------|\n",
    "| Feature type | Binary (0/1, yes/no) | Count-based (0, 1, 2, ...) |\n",
    "| Used for | Presence/absence | Frequency of terms |\n",
    "| Example | \"Does this email contain the word 'free'?\" | \"How many times does 'free' appear?\" |\n",
    "| Smoothing | Usually Laplace | Laplace or Lidstone |\n",
    "\n",
    "‚úÖ Use **Bernoulli NB** when features are **binary**.  \n",
    "‚úÖ Use **Multinomial NB** when features are **counts or frequencies** (like word counts in NLP).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. How does Bernoulli Naive Bayes handle missing values?**\n",
    "\n",
    "Bernoulli Naive Bayes **does not natively handle missing values**. If a feature value is missing:\n",
    "\n",
    "- ‚ùå It treats missing data as **zero** (which may imply ‚Äúabsence‚Äù of a feature).\n",
    "- ‚úÖ The **recommended approach** is to **impute** missing values before applying the model:\n",
    "  - For binary features: fill with mode (most frequent value) or use a placeholder.\n",
    "  - Or, use scikit-learn‚Äôs `SimpleImputer` to handle it prior to modeling.\n",
    "\n",
    "So you should always **clean or impute** before feeding data into Bernoulli NB.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. Can Gaussian Naive Bayes be used for multi-class classification?**\n",
    "\n",
    "‚úÖ **Yes, absolutely!**\n",
    "\n",
    "**Gaussian Naive Bayes (GNB)** is **multi-class by design**. It calculates the probability of each class based on:\n",
    "\n",
    "\\[\n",
    "P(C_k | x) \\propto P(C_k) \\prod_{i=1}^n P(x_i | C_k)\n",
    "\\]\n",
    "\n",
    "- Each class \\( C_k \\) gets its own **mean** and **variance** for each feature (assuming a Gaussian distribution).\n",
    "- During prediction, the model outputs the class with the **highest posterior probability**.\n",
    "\n",
    "üí° In `scikit-learn`, `GaussianNB` handles multi-class out-of-the-box using the **one-vs-rest** strategy internally.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050659e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
