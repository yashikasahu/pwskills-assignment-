{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f396c109",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "**Min-Max scaling** is a technique used in data preprocessing to **rescale the values of a feature to a fixed range**, usually **0 to 1**. It ensures that all features contribute equally during model training, especially for distance-based algorithms like KNN or gradient descent-based ones like linear regression.\n",
    "\n",
    "#### üìå How it works:\n",
    "Each value is scaled using the formula:\n",
    "\\[\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "\\]\n",
    "\n",
    "#### üîç Example:\n",
    "Let's say you have house sizes (in square feet): `[1000, 1500, 2000, 2500, 3000]`\n",
    "\n",
    "- Min = 1000, Max = 3000  \n",
    "- Apply Min-Max scaling:\n",
    "\n",
    "```python\n",
    "scaled = [(x - 1000) / (3000 - 1000) for x in [1000, 1500, 2000, 2500, 3000]]\n",
    "# Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "```\n",
    "\n",
    "Now all the values are within **[0, 1]**, making the feature more comparable to others during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "**Unit Vector scaling**, also known as **vector normalization**, scales the entire feature vector (row) so that its **length (or norm)** is 1. It's useful when the **direction** of the data matters more than the magnitude ‚Äî often in text classification or clustering.\n",
    "\n",
    "#### üìå Formula:\n",
    "For a feature vector \\(\\vec{x} = [x_1, x_2, ..., x_n]\\):\n",
    "\\[\n",
    "\\vec{x}_{\\text{normalized}} = \\frac{\\vec{x}}{||\\vec{x}||}\n",
    "\\]\n",
    "Where \\(||\\vec{x}||\\) is the Euclidean norm: \\(\\sqrt{x_1^2 + x_2^2 + ... + x_n^2}\\)\n",
    "\n",
    "#### üîç Example:\n",
    "If you have a data point with features `[3, 4]`:\n",
    "\n",
    "- Euclidean norm = ‚àö(3¬≤ + 4¬≤) = ‚àö25 = 5\n",
    "- Normalized vector: `[3/5, 4/5] = [0.6, 0.8]`\n",
    "\n",
    "#### üîÑ Difference from Min-Max:\n",
    "- **Min-Max** scales feature-wise (column-wise) to a specific range.\n",
    "- **Unit Vector** scales instance-wise (row-wise) so each row has a length of 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "**PCA** is a technique used to **reduce the number of features** in a dataset while keeping as much **important (variance-based) information** as possible. It transforms the original features into a new set of variables called **principal components**, which are **uncorrelated** and ordered by the amount of variance they capture.\n",
    "\n",
    "#### üìå Key idea:\n",
    "- PCA projects the data into a new space with **fewer dimensions**.\n",
    "- Useful when you have **many correlated features** or want to speed up training.\n",
    "\n",
    "#### üîç Example:\n",
    "Let‚Äôs say you have data on:\n",
    "- `Height`, `Weight`, and `BMI`\n",
    "\n",
    "Since BMI is derived from height and weight, these features are likely **correlated**. PCA can combine them into **1 or 2 principal components** that retain most of the information, reducing redundancy.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(original_data)\n",
    "```\n",
    "\n",
    "Now you‚Äôve gone from 3 features down to 2 while preserving most of the data's variance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7e401",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "**Feature Extraction** is the process of creating new features from existing ones to better capture the patterns in the data. **PCA (Principal Component Analysis)** is a powerful method of feature extraction because it transforms the original features into a new set of **uncorrelated variables** called **principal components**, which capture the **maximum variance** in the data.\n",
    "\n",
    "#### üîç How PCA works as Feature Extraction:\n",
    "Instead of selecting existing features (like in feature selection), PCA **creates new features** ‚Äî these are combinations (linear transformations) of the original ones.\n",
    "\n",
    "#### ‚úÖ Example:\n",
    "Imagine a dataset with:\n",
    "- `Feature A` = number of items sold\n",
    "- `Feature B` = revenue\n",
    "- These are highly correlated (more items sold ‚Üí more revenue)\n",
    "\n",
    "PCA might combine these into a new component:\n",
    "- `PC1 = 0.7*A + 0.7*B` ‚Üí capturing the \"sales performance\"\n",
    "- This new feature (PC1) is more compact and still carries the main pattern from A and B.\n",
    "\n",
    "By using PCA, you reduce dimensionality **and** extract more meaningful, uncorrelated features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "When building a recommendation system, features like **price**, **rating**, and **delivery time** might be on **very different scales**. For example:\n",
    "- Price might range from $5 to $50\n",
    "- Rating is from 1 to 5\n",
    "- Delivery time might be in minutes, say 10 to 90\n",
    "\n",
    "If you don‚Äôt scale them, the model may incorrectly give more weight to features with larger numbers (like delivery time).\n",
    "\n",
    "#### ‚úÖ How to use Min-Max Scaling:\n",
    "1. For each feature (price, rating, time), apply:\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "   \\]\n",
    "\n",
    "2. This will rescale all values to **[0, 1]**, putting them on an equal footing.\n",
    "\n",
    "#### üîç Example in Python:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[['price', 'rating', 'delivery_time']])\n",
    "```\n",
    "\n",
    "Now, your model can treat all features **fairly**, and comparisons between restaurants will be more balanced.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "In financial data, you often deal with **high-dimensional data** ‚Äî tons of indicators, ratios, moving averages, sector performance, etc. Many of these are **correlated**, which can lead to overfitting and slow training.\n",
    "\n",
    "#### ‚úÖ How to use PCA for dimensionality reduction:\n",
    "1. **Standardize** the dataset (using `StandardScaler`) to ensure each feature contributes equally.\n",
    "2. Apply **PCA** to the standardized data.\n",
    "3. Decide how many components to keep by checking the **explained variance ratio** ‚Äî you might keep enough components to capture **95% of the variance**.\n",
    "4. Use these principal components as input to your prediction model.\n",
    "\n",
    "#### üîç Example:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(financial_data)\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "Now you‚Äôve reduced the complexity of your dataset while retaining the most important signals ‚Äî making the model **faster** and potentially **more accurate**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c91a74",
   "metadata": {},
   "source": [
    "### Q7: Min-Max Scaling to Transform the Values to a Range of -1 to 1\n",
    "\n",
    "Min-Max scaling is used to transform the features of a dataset into a specific range. The formula for Min-Max scaling is:\n",
    "\n",
    "\\[\n",
    "\\text{Scaled value} = \\frac{(X - X_{\\text{min}})}{(X_{\\text{max}} - X_{\\text{min}})} \\times (\\text{new max} - \\text{new min}) + \\text{new min}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( X \\) is the original value.\n",
    "- \\( X_{\\text{min}} \\) is the minimum value in the original dataset.\n",
    "- \\( X_{\\text{max}} \\) is the maximum value in the original dataset.\n",
    "- new max = 1, new min = -1 (since we are scaling to a range of -1 to 1).\n",
    "\n",
    "Given the dataset: \\([1, 5, 10, 15, 20]\\), let's perform Min-Max scaling:\n",
    "\n",
    "- \\( X_{\\text{min}} = 1 \\)\n",
    "- \\( X_{\\text{max}} = 20 \\)\n",
    "\n",
    "Now, applying the formula for each data point:\n",
    "\n",
    "\\[\n",
    "\\text{Scaled value} = \\frac{(X - 1)}{(20 - 1)} \\times (1 - (-1)) + (-1)\n",
    "\\]\n",
    "\\[\n",
    "\\text{Scaled value} = \\frac{(X - 1)}{19} \\times 2 - 1\n",
    "\\]\n",
    "\n",
    "Let‚Äôs calculate this for each value in the dataset:\n",
    "\n",
    "- For \\( X = 1 \\):\n",
    "  \\[\n",
    "  \\text{Scaled value} = \\frac{(1 - 1)}{19} \\times 2 - 1 = 0 - 1 = -1\n",
    "  \\]\n",
    "\n",
    "- For \\( X = 5 \\):\n",
    "  \\[\n",
    "  \\text{Scaled value} = \\frac{(5 - 1)}{19} \\times 2 - 1 = \\frac{4}{19} \\times 2 - 1 \\approx 0.4211 - 1 = -0.5789\n",
    "  \\]\n",
    "\n",
    "- For \\( X = 10 \\):\n",
    "  \\[\n",
    "  \\text{Scaled value} = \\frac{(10 - 1)}{19} \\times 2 - 1 = \\frac{9}{19} \\times 2 - 1 \\approx 0.9474 - 1 = -0.0526\n",
    "  \\]\n",
    "\n",
    "- For \\( X = 15 \\):\n",
    "  \\[\n",
    "  \\text{Scaled value} = \\frac{(15 - 1)}{19} \\times 2 - 1 = \\frac{14}{19} \\times 2 - 1 \\approx 1.4737 - 1 = 0.4737\n",
    "  \\]\n",
    "\n",
    "- For \\( X = 20 \\):\n",
    "  \\[\n",
    "  \\text{Scaled value} = \\frac{(20 - 1)}{19} \\times 2 - 1 = \\frac{19}{19} \\times 2 - 1 = 2 - 1 = 1\n",
    "  \\]\n",
    "\n",
    "Thus, the transformed values are:\n",
    "\n",
    "\\[\n",
    "[-1, -0.5789, -0.0526, 0.4737, 1]\n",
    "\\]\n",
    "\n",
    "### Q8: Feature Extraction Using PCA (Principal Component Analysis)\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset while retaining as much of the variance as possible. The number of principal components you choose to retain depends on how much variance you want to explain from the original features.\n",
    "\n",
    "The steps in PCA are:\n",
    "\n",
    "1. **Standardize** the data (mean 0 and variance 1 for each feature).\n",
    "2. Compute the **covariance matrix** of the features.\n",
    "3. Calculate the **eigenvalues** and **eigenvectors** of the covariance matrix.\n",
    "4. Sort the eigenvalues in descending order, and the corresponding eigenvectors are the principal components.\n",
    "5. Choose the top \\( k \\) principal components based on the eigenvalues, which represent the variance explained by each component.\n",
    "\n",
    "To decide how many principal components to retain:\n",
    "\n",
    "1. **Eigenvalue Criterion**: Retain the components with the highest eigenvalues, as they explain the most variance in the data.\n",
    "2. **Cumulative Variance Criterion**: Retain enough components so that the cumulative variance explained by the retained components is high (often, 80-90% is considered sufficient).\n",
    "\n",
    "If we have the following features: **height, weight, age, gender, and blood pressure** (5 features), PCA would typically produce up to 5 principal components (one for each feature). However, the number of principal components to retain depends on the amount of variance you want to preserve.\n",
    "\n",
    "**How many components should be retained?**\n",
    "- If we want to retain **90% of the variance**, we will examine the eigenvalues and cumulative explained variance. \n",
    "- If the first few principal components explain a large proportion of the variance, you might only need to keep the first 2 or 3 components.\n",
    "\n",
    "In practice, you would calculate the eigenvalues and plot the **scree plot** (a plot of eigenvalues). You look for the \"elbow\" point, where the eigenvalues start to decrease more slowly. The number of components before the elbow is often the best choice.\n",
    "\n",
    "**In summary:**\n",
    "- Retain enough principal components such that the cumulative explained variance is **90%** or more (commonly used threshold).\n",
    "- If the first 2-3 components explain most of the variance, it may be reasonable to retain only those 2-3 principal components.\n",
    "\n",
    "If you perform PCA on this dataset, the decision is made based on the explained variance of each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b02b63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
