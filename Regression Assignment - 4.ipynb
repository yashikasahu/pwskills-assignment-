{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5cdab",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. What Is Lasso Regression and How Does It Differ from Other Regression Techniques?**\n",
    "\n",
    "**Lasso Regression (Least Absolute Shrinkage and Selection Operator)** is a **linear regression model** that uses **L1 regularization** to penalize the absolute size of the coefficients.\n",
    "\n",
    "#### üßÆ Loss Function:\n",
    "\\[\n",
    "\\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |\\beta_i|\n",
    "\\]\n",
    "\n",
    "#### üîç Key Differences:\n",
    "- **Lasso vs. OLS**: Lasso adds a penalty term, OLS does not.\n",
    "- **Lasso vs. Ridge**: \n",
    "  - Ridge uses L2 penalty (squares of coefficients) ‚Üí shrinks coefficients, but doesn‚Äôt make them zero.\n",
    "  - **Lasso can shrink coefficients to exactly zero**, effectively **removing features**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. What Is the Main Advantage of Using Lasso Regression in Feature Selection?**\n",
    "\n",
    "‚úÖ **Automatic feature selection.**\n",
    "\n",
    "- Lasso can set **some coefficients exactly to 0**, which means those features are **excluded** from the model.\n",
    "- Helps simplify models and reduce overfitting, especially when:\n",
    "  - You have **many predictors**.\n",
    "  - You suspect some predictors are **irrelevant or redundant**.\n",
    "\n",
    "> üß† Think of Lasso as a model that **learns which features to ignore** ‚Äî very useful when working with high-dimensional datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. How Do You Interpret the Coefficients of a Lasso Regression Model?**\n",
    "\n",
    "- **Non-zero coefficient**: That feature is contributing to the prediction. You can interpret it similarly to OLS (change in target per 1-unit change in feature).\n",
    "- **Zero coefficient**: That feature was deemed unimportant and **excluded** from the model.\n",
    "- **Smaller magnitude**: Feature has weaker influence (but is still in the model).\n",
    "\n",
    "‚ö†Ô∏è Reminder: Since Lasso adds regularization, the coefficients are **biased** (shrunken) and shouldn‚Äôt be interpreted the same way as OLS for statistical inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What Are the Tuning Parameters in Lasso Regression and How Do They Affect Performance?**\n",
    "\n",
    "The primary tuning parameter in Lasso is:\n",
    "\n",
    "### üîß **Lambda (also called alpha in `scikit-learn`)**:\n",
    "Controls the strength of the L1 penalty.\n",
    "\n",
    "- **Small Œª (close to 0)** ‚Üí Less regularization ‚Üí More features included.\n",
    "- **Large Œª** ‚Üí Stronger penalty ‚Üí More coefficients shrink to zero ‚Üí More feature selection.\n",
    "\n",
    "#### üß™ How to Choose Œª:\n",
    "- Use **cross-validation** (e.g., `LassoCV` in `scikit-learn`).\n",
    "- Common to try a range of values on a **log scale**.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV(cv=5).fit(X_train, y_train)\n",
    "print(\"Best alpha:\", model.alpha_)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67769f31",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q5. Can Lasso Regression Be Used for Non-Linear Regression Problems?**\n",
    "\n",
    "‚úÖ **Yes ‚Äî but not directly.**\n",
    "\n",
    "Lasso itself is a **linear model**, but you can handle **non-linear relationships** by **transforming your features** before applying Lasso.\n",
    "\n",
    "#### üîß How to make Lasso work for non-linear problems:\n",
    "1. **Add polynomial features**:\n",
    "   - E.g., instead of just `x`, include `x¬≤`, `x¬≥`, etc.\n",
    "   - Use `PolynomialFeatures` from `sklearn.preprocessing`.\n",
    "\n",
    "2. **Use interaction terms**:\n",
    "   - Include terms like `x1 * x2` to capture interaction effects.\n",
    "\n",
    "3. **Use kernel tricks or basis expansions**:\n",
    "   - Custom transformations that capture curvature.\n",
    "\n",
    "> After transforming, apply **Lasso Regression** on the new feature space. It will still do feature selection, even with polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What‚Äôs the Difference Between Ridge Regression and Lasso Regression?**\n",
    "\n",
    "| Feature | **Ridge Regression** | **Lasso Regression** |\n",
    "|--------|----------------------|----------------------|\n",
    "| **Penalty** | L2 (squared coefficients) | L1 (absolute values) |\n",
    "| **Effect** | Shrinks coefficients | Shrinks & sets some to 0 |\n",
    "| **Feature Selection** | ‚ùå No (keeps all) | ‚úÖ Yes (automatic selection) |\n",
    "| **Best for** | Multicollinearity, all features useful | High-dimensional data, feature reduction |\n",
    "| **Behavior with correlated features** | Distributes weight | Selects one, drops others |\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. Can Lasso Handle Multicollinearity in Input Features?**\n",
    "\n",
    "‚úÖ Yes, but differently than Ridge.\n",
    "\n",
    "- **Ridge**: Shares the weight across correlated features (keeps them all with smaller coefficients).\n",
    "- **Lasso**: Often **picks one** feature and **sets the rest to zero**.\n",
    "\n",
    "This is helpful for interpretability but can be **unstable** ‚Äî small data changes might make it choose a different variable from a correlated group.\n",
    "\n",
    "> For a more balanced approach, **Elastic Net** (L1 + L2) is great ‚Äî it handles multicollinearity better than Lasso alone.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. How Do You Choose the Optimal Œª (Lambda) in Lasso Regression?**\n",
    "\n",
    "Just like Ridge, the key is **cross-validation**.\n",
    "\n",
    "#### üîç Steps:\n",
    "1. Use `LassoCV` or `GridSearchCV` to search over a range of lambda values.\n",
    "2. Evaluate using a performance metric (e.g., RMSE or MAE).\n",
    "3. Select the Œª that **minimizes validation error**.\n",
    "\n",
    "#### Example in Python:\n",
    "```python\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Best lambda (alpha):\", lasso.alpha_)\n",
    "```\n",
    "\n",
    "‚úÖ **Tip**: Use a **logarithmic scale** for alphas ‚Äî e.g., from `1e-4` to `1e2`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60cb2b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
