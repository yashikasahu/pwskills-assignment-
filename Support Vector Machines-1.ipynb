{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5324fc3",
   "metadata": {},
   "source": [
    "You've got a packed and exciting SVM-focused project here â€” covering both **theory** and **hands-on implementation**. Let's go through it step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Q1. Mathematical Formula for a Linear SVM\n",
    "\n",
    "A **Linear SVM** aims to find a hyperplane that separates data into two classes. The decision function is:\n",
    "\n",
    "\\[\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{x} \\) is the input feature vector\n",
    "- \\( \\mathbf{w} \\) is the weight vector (normal to the hyperplane)\n",
    "- \\( b \\) is the bias or intercept\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Q2. Objective Function of a Linear SVM\n",
    "\n",
    "For a **hard-margin SVM** (linearly separable data), we want to:\n",
    "\n",
    "\\[\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "\\]\n",
    "Subject to:\n",
    "\\[\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "For **soft-margin SVM** (non-separable data), we introduce slack variables \\( \\xi_i \\):\n",
    "\n",
    "\\[\n",
    "\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "\\]\n",
    "Subject to:\n",
    "\\[\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i,\\quad \\xi_i \\geq 0\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Q3. What is the Kernel Trick in SVM?\n",
    "\n",
    "The **kernel trick** allows SVMs to operate in a **higher-dimensional feature space** without explicitly transforming the data.\n",
    "\n",
    "Instead of computing the dot product in high dimensions, we compute a kernel function:\n",
    "\n",
    "\\[\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\n",
    "\\]\n",
    "\n",
    "Common kernels:\n",
    "- Linear: \\( K(x, x') = x^\\top x' \\)\n",
    "- Polynomial: \\( K(x, x') = (x^\\top x' + 1)^d \\)\n",
    "- RBF (Gaussian): \\( K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Q4.What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "**Support Vectors** are the data points closest to the hyperplane â€” they lie **on or within the margin**.\n",
    "\n",
    "They **define** the decision boundary. Removing non-support vectors won't change the model, but removing support vectors will.\n",
    "\n",
    "Example:\n",
    "- If a margin is defined by 3 points, changing or removing them alters the hyperplane entirely.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ…Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "I'll generate **graphs** to show:\n",
    "- Hyperplane\n",
    "- Marginal Planes\n",
    "- Hard Margin\n",
    "- Soft Margin\n",
    "\n",
    "Letâ€™s generate them now ðŸ‘‡\n",
    "\n",
    "\n",
    "\n",
    "Here are two visualizations:\n",
    "\n",
    "- **Left: Hard Margin SVM** â€” maximum margin with no tolerance for misclassification. Works only when data is perfectly separable.\n",
    "- **Right: Soft Margin SVM** â€” allows some misclassification (slack) for better generalization with real-world noisy data.\n",
    "\n",
    "In both:\n",
    "- **Solid black line** = Decision boundary (hyperplane)\n",
    "- **Dashed lines** = Marginal planes\n",
    "- **Circled points** = Support vectors â€” critical for defining the margin\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Up Next:\n",
    "**Q6: SVM Implementation using the Iris Dataset**  \n",
    "Then the **Bonus**: Custom SVM from scratch and comparing it to scikit-learn's.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic linearly separable dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, class_sep=2, random_state=42)\n",
    "y = 2 * y - 1  # Convert labels from {0,1} to {-1,1}\n",
    "\n",
    "# Fit a hard-margin SVM\n",
    "clf_hard = svm.SVC(kernel='linear', C=1e10)  # effectively hard margin\n",
    "clf_hard.fit(X, y)\n",
    "\n",
    "# Fit a soft-margin SVM\n",
    "clf_soft = svm.SVC(kernel='linear', C=1.0)\n",
    "clf_soft.fit(X, y)\n",
    "\n",
    "def plot_svm(clf, title, subplot):\n",
    "    ax = plt.subplot(1, 2, subplot)\n",
    "    ax.set_title(title)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=30)\n",
    "\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], \n",
    "               s=100, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    w = clf.coef_[0]\n",
    "    b = clf.intercept_[0]\n",
    "    x_plot = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "    y_decision = -(w[0] * x_plot + b) / w[1]\n",
    "    y_margin1 = -(w[0] * x_plot + b - 1) / w[1]\n",
    "    y_margin2 = -(w[0] * x_plot + b + 1) / w[1]\n",
    "\n",
    "    ax.plot(x_plot, y_decision, 'k-')\n",
    "    ax.plot(x_plot, y_margin1, 'k--')\n",
    "    ax.plot(x_plot, y_margin2, 'k--')\n",
    "    ax.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plot_svm(clf_hard, \"Hard Margin SVM\", 1)\n",
    "plot_svm(clf_soft, \"Soft Margin SVM\", 2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507c89b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
