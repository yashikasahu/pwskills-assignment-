{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad54d4f0",
   "metadata": {},
   "source": [
    "R-squared (Coefficient of Determination) measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
    "\n",
    "📏 Formula:\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "𝑆\n",
    "𝑆\n",
    "res\n",
    "𝑆\n",
    "𝑆\n",
    "tot\n",
    "R \n",
    "2\n",
    " =1− \n",
    "SS \n",
    "tot\n",
    "​\n",
    " \n",
    "SS \n",
    "res\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "Where:\n",
    "\n",
    "𝑆\n",
    "𝑆\n",
    "res\n",
    "SS \n",
    "res\n",
    "​\n",
    "  = Sum of Squares of Residuals = \n",
    "∑\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "∑(y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "𝑆\n",
    "𝑆\n",
    "tot\n",
    "SS \n",
    "tot\n",
    "​\n",
    "  = Total Sum of Squares = \n",
    "∑\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "ˉ\n",
    ")\n",
    "2\n",
    "∑(y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "ˉ\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "🎯 Interpretation:\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "0\n",
    "R \n",
    "2\n",
    " =0: Model explains none of the variance.\n",
    "\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "R \n",
    "2\n",
    " =1: Model explains all the variance.\n",
    "\n",
    "Example: \n",
    "𝑅\n",
    "2\n",
    "=\n",
    "0.85\n",
    "R \n",
    "2\n",
    " =0.85 → 85% of the variability in \n",
    "𝑦\n",
    "y is explained by the model.\n",
    "\n",
    "⚠️ Note:\n",
    "R-squared can increase with more predictors — even if they aren't meaningful.\n",
    "\n",
    "Q2. What Is Adjusted R-squared and How Is It Different?\n",
    "Adjusted R-squared compensates for the fact that R-squared always increases with more variables. It penalizes for unnecessary predictors.\n",
    "\n",
    "🧮 Formula:\n",
    "Adjusted \n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑅\n",
    "2\n",
    ")\n",
    "(\n",
    "𝑛\n",
    "−\n",
    "1\n",
    ")\n",
    "𝑛\n",
    "−\n",
    "𝑘\n",
    "−\n",
    "1\n",
    ")\n",
    "Adjusted R \n",
    "2\n",
    " =1−( \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )(n−1)\n",
    "​\n",
    " )\n",
    "Where:\n",
    "\n",
    "𝑛\n",
    "n = number of observations\n",
    "\n",
    "𝑘\n",
    "k = number of predictors\n",
    "\n",
    "🚀 Why It's Better:\n",
    "Reflects the true explanatory power of the model.\n",
    "\n",
    "Can decrease when irrelevant variables are added.\n",
    "\n",
    "Q3. When Should You Use Adjusted R-squared?\n",
    "Use Adjusted R-squared when:\n",
    "\n",
    "You are working with multiple predictors.\n",
    "\n",
    "You're comparing different models with a different number of predictors.\n",
    "\n",
    "You want a more reliable measure of model quality.\n",
    "\n",
    "R-squared is fine for simple linear regression (1 predictor). But when you have multiple variables, adjusted R-squared gives a truer picture.\n",
    "\n",
    "Q4. RMSE, MSE, MAE — What Do They Mean in Regression?\n",
    "These are error metrics to evaluate how well your regression model is performing.\n",
    "\n",
    "1. MSE (Mean Squared Error):\n",
    "MSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑(y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "Squares the errors — penalizes large errors more heavily.\n",
    "\n",
    "Used for optimization (e.g., in gradient descent).\n",
    "\n",
    "2. RMSE (Root Mean Squared Error):\n",
    "RMSE\n",
    "=\n",
    "MSE\n",
    "RMSE= \n",
    "MSE\n",
    "​\n",
    " \n",
    "Same units as the target variable → easier to interpret.\n",
    "\n",
    "Popular for evaluating regression models.\n",
    "\n",
    "3. MAE (Mean Absolute Error):\n",
    "MAE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "∣\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑∣y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Measures average magnitude of errors, without squaring.\n",
    "\n",
    "Less sensitive to outliers than MSE/RMSE.\n",
    "\n",
    "🧠 Quick Summary Table:\n",
    "Metric\tPenalizes Large Errors?\tSame Units as Target?\tOutlier Sensitivity\n",
    "MSE\tYes (squared error)\tNo\tHigh\n",
    "RMSE\tYes\tYes\tHigh\n",
    "MAE\tNo\tYes\tLower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8814f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q5. Advantages and Disadvantages of RMSE, MSE, and MAE**\n",
    "\n",
    "| Metric | Advantages | Disadvantages |\n",
    "|--------|------------|---------------|\n",
    "| **MSE** | - Useful for penalizing large errors<br>- Mathematically convenient (used in optimization algorithms)<br>- Differentiable (good for gradient descent) | - Not in same unit as target<br>- Heavily influenced by outliers |\n",
    "| **RMSE** | - Same unit as target variable (more interpretable)<br>- Penalizes large errors (more sensitive) | - Still very sensitive to outliers<br>- Harder to optimize than MSE |\n",
    "| **MAE** | - Robust to outliers<br>- Same unit as target<br>- Easy to interpret as average error | - Not differentiable at zero (can be an issue in optimization)<br>- Doesn’t emphasize large errors (which might matter in some domains) |\n",
    "\n",
    "✅ **Use MAE** when:\n",
    "- You want a **robust** metric.\n",
    "- You care equally about all errors.\n",
    "\n",
    "✅ **Use RMSE or MSE** when:\n",
    "- You want to **emphasize large errors** (e.g., in medical or financial predictions).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What Is Lasso Regularization? How Does It Differ from Ridge?**\n",
    "\n",
    "**Lasso (Least Absolute Shrinkage and Selection Operator)** adds an **L1 penalty** to the loss function, which can shrink coefficients to **exactly zero**.\n",
    "\n",
    "#### 🔢 Lasso Cost Function:\n",
    "\\[\n",
    "\\text{Loss} = \\text{RSS} + \\lambda \\sum |\\beta_i|\n",
    "\\]\n",
    "\n",
    "**Ridge Regression** adds an **L2 penalty** — it shrinks coefficients **toward zero**, but not exactly zero.\n",
    "\n",
    "#### 🔢 Ridge Cost Function:\n",
    "\\[\n",
    "\\text{Loss} = \\text{RSS} + \\lambda \\sum \\beta_i^2\n",
    "\\]\n",
    "\n",
    "#### 🎯 Key Difference:\n",
    "- **Lasso**: Can **select variables** by shrinking some coefficients to zero → **feature selection**.\n",
    "- **Ridge**: Keeps all variables but reduces their influence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. How Do Regularized Linear Models Prevent Overfitting?**\n",
    "\n",
    "Overfitting happens when a model learns noise in the training data. Regularization **constrains model complexity** by penalizing large coefficients.\n",
    "\n",
    "#### 🔁 Example:\n",
    "Suppose you're predicting house prices using 50 features.\n",
    "\n",
    "- **Linear Regression**: May overfit by assigning large weights to unimportant variables.\n",
    "- **Ridge/Lasso**: Shrinks coefficients → forces model to generalize better.\n",
    "\n",
    "**Visual Analogy**: Think of regularization as adding “friction” so the model can’t make wild swings (huge weights) just to fit every single point.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. Limitations of Regularized Linear Models**\n",
    "\n",
    "#### ❌ Limitations:\n",
    "1. **Linearity Assumption**: They still assume a linear relationship.\n",
    "2. **Feature Engineering Needed**: Can’t capture complex interactions or nonlinear patterns on their own.\n",
    "3. **Hyperparameter Tuning**: The regularization strength \\( \\lambda \\) must be carefully chosen (e.g., via cross-validation).\n",
    "4. **Lasso Limit**: When predictors are highly correlated, Lasso may arbitrarily pick one and drop the others.\n",
    "\n",
    "#### 🧠 When Not to Use:\n",
    "- If your data has strong **non-linear relationships** → consider tree-based models (e.g., Random Forest, XGBoost).\n",
    "- If interpretability isn't a priority and performance is more important → use more complex models like neural networks.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
