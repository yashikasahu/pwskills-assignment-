{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c104c9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. What is Gradient Boosting Regression?**\n",
    "\n",
    "**Gradient Boosting Regression** is a machine learning technique used for **predicting continuous values** by building an ensemble of **decision trees**, each one learning to correct the mistakes of the previous ones.\n",
    "\n",
    "Think of it like this:\n",
    "- Start with a **bad guess** (like predicting the average for everyone).\n",
    "- See where you’re going wrong.\n",
    "- Build a tiny model (a *weak learner*) that tries to fix those errors.\n",
    "- Repeat this process—each time focusing on the **residuals** (leftover errors).\n",
    "- Gradually, you get a model that’s really good because it keeps learning from its own mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. Implement a simple Gradient Boosting algorithm from scratch using Python and NumPy**\n",
    "\n",
    "Here’s a basic implementation to give you the idea:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate dummy data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.5, 5.0, 6.0, 8.5])\n",
    "\n",
    "# Parameters\n",
    "n_estimators = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions with the mean\n",
    "pred = np.full_like(y, y.mean(), dtype=float)\n",
    "models = []\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    residuals = y - pred\n",
    "\n",
    "    # Fit a simple model: decision stump (1-feature linear regression here)\n",
    "    coef = np.sum((X.flatten() - X.mean()) * residuals) / np.sum((X.flatten() - X.mean())**2)\n",
    "    intercept = residuals.mean() - coef * X.mean()\n",
    "    prediction_update = coef * X.flatten() + intercept\n",
    "    \n",
    "    # Update prediction\n",
    "    pred += learning_rate * prediction_update\n",
    "    models.append((coef, intercept))\n",
    "\n",
    "# Evaluate\n",
    "print(\"Final Predictions:\", pred)\n",
    "print(\"MSE:\", mean_squared_error(y, pred))\n",
    "print(\"R2 Score:\", r2_score(y, pred))\n",
    "```\n",
    "\n",
    "✔️ This is **not using trees**—just a simple linear update—but it shows the logic of how gradient boosting stacks learners to reduce error.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. Hyperparameter Tuning (Learning Rate, Trees, Depth)**\n",
    "\n",
    "In real scenarios, we tune these:\n",
    "- **Learning rate**: How much correction each tree applies.\n",
    "- **Number of trees**: More trees = better fit (to a point).\n",
    "- **Max depth**: How complex each tree is.\n",
    "\n",
    "Use **Grid Search** or **Random Search** from `sklearn.model_selection` like so:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What is a weak learner in Gradient Boosting?**\n",
    "\n",
    "A **weak learner** is a model that’s **just slightly better than guessing**. In boosting, these are often **shallow decision trees** (usually depth-1 or depth-2).\n",
    "\n",
    "Why weak?\n",
    "- Because they’re fast.\n",
    "- And stacking lots of weak models avoids overfitting (compared to a single complex model).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. What’s the intuition behind Gradient Boosting?**\n",
    "\n",
    "The idea is:\n",
    "1. Start with a rough guess (initial model).\n",
    "2. Look at what you got wrong (residuals).\n",
    "3. Train a model to fix those errors.\n",
    "4. Add this model’s prediction to your overall model.\n",
    "5. Repeat.\n",
    "\n",
    "Each model is “boosting” the performance by focusing on previous errors. You’re literally walking **down the gradient of the loss function**, hence the name.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. How does Gradient Boosting build an ensemble of weak learners?**\n",
    "\n",
    "It builds them **sequentially**:\n",
    "- Model 1 → predicts a baseline.\n",
    "- Model 2 → trained on the errors of Model 1.\n",
    "- Model 3 → trained on the errors of Model 1 + 2.\n",
    "- And so on...\n",
    "\n",
    "Each tree adds a **small correction** to the previous prediction, slowly converging to the best fit.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. What are the steps in constructing the mathematical intuition of Gradient Boosting?**\n",
    "\n",
    "Here’s the general process, a bit simplified:\n",
    "\n",
    "1. **Start with a constant model** \\( F_0(x) \\), like the mean of the target.\n",
    "2. For each iteration \\( m \\):\n",
    "   - Compute the **negative gradient** of the loss function → this is your \"pseudo-residual\".\n",
    "   - Fit a weak learner \\( h_m(x) \\) to these residuals.\n",
    "   - Update the model:\n",
    "     \\[\n",
    "     F_m(x) = F_{m-1}(x) + \\alpha \\cdot h_m(x)\n",
    "     \\]\n",
    "     where \\( \\alpha \\) is the learning rate.\n",
    "\n",
    "This approach is like **gradient descent in function space**—you’re minimizing loss by improving the model iteratively.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13096bf9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
