{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140e6954",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Q1. Describe the Decision Tree Classifier Algorithm**\n",
    "\n",
    "A **Decision Tree Classifier** is a supervised learning algorithm used for classification (and regression). It splits the data into subsets based on feature values using **decision rules**, forming a **tree structure**.\n",
    "\n",
    "#### **How it works:**\n",
    "1. Starts at the **root node** with the full dataset.\n",
    "2. Selects the **best feature** to split the data (based on a criterion like Gini impurity or Information Gain).\n",
    "3. Creates **branches** for each value of the feature.\n",
    "4. Repeats the process **recursively** until:\n",
    "   - Maximum depth is reached\n",
    "   - Nodes are pure (only one class left)\n",
    "   - No further gain can be made\n",
    "\n",
    "It makes predictions by **traversing** the tree from root to leaf based on the input features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. Step-by-step Mathematical Intuition of Decision Tree Classification**\n",
    "\n",
    "1. **Select a Split Feature:**  \n",
    "   Use a metric like **Information Gain** or **Gini Impurity** to choose the best feature to split:\n",
    "   - **Entropy:**  \n",
    "     \\[\n",
    "     H(S) = - \\sum_{i=1}^c p_i \\log_2 p_i\n",
    "     \\]\n",
    "   - **Information Gain:**  \n",
    "     \\[\n",
    "     IG = H(parent) - \\sum_{k} \\frac{|child_k|}{|parent|} H(child_k)\n",
    "     \\]\n",
    "   - **Gini Index:**  \n",
    "     \\[\n",
    "     Gini = 1 - \\sum_{i=1}^c p_i^2\n",
    "     \\]\n",
    "\n",
    "2. **Split the Dataset:**  \n",
    "   Based on the best split, partition the dataset.\n",
    "\n",
    "3. **Repeat Recursively:**  \n",
    "   Do the same for each child node until the stopping condition.\n",
    "\n",
    "4. **Prediction:**  \n",
    "   Follow the tree path using feature values, and predict the **class label at the leaf node**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. Using a Decision Tree for Binary Classification**\n",
    "\n",
    "#### **Example Problem:** Spam Detection (Spam vs. Not Spam)\n",
    "\n",
    "**Features:** Contains \"Buy\", Number of links, Sender domain  \n",
    "**Target:** Spam (1) or Not Spam (0)\n",
    "\n",
    "- Tree will split emails by:\n",
    "  1. Whether it contains \"Buy\"\n",
    "  2. Number of links > 3\n",
    "  3. Sender domain = \"free.com\"\n",
    "\n",
    "Each path in the tree leads to a classification. The final leaf node gives either **Spam** or **Not Spam**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. Geometric Intuition Behind Decision Tree Classification**\n",
    "\n",
    "Decision trees create **axis-aligned splits** in the feature space. The geometric interpretation is:\n",
    "\n",
    "- It partitions the space into **rectangles (or hyperrectangles)**.\n",
    "- Each region corresponds to a **class label**.\n",
    "- Example in 2D: A tree may split:\n",
    "  - `Age > 30` â†’ right\n",
    "  - `Salary < 50K` â†’ left\n",
    "  - Thus forming rectangular decision boundaries in the Age-Salary space.\n",
    "\n",
    "âœ… Very intuitive for visualizing simple 2D/3D datasets!\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. Define Confusion Matrix and Its Use**\n",
    "\n",
    "A **confusion matrix** is a table used to evaluate the performance of a classification model.\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP)  | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "It gives a **complete picture** of model performance beyond just accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. Example of Confusion Matrix and Metric Calculations**\n",
    "\n",
    "**Example:**\n",
    "```plaintext\n",
    "               Predicted\n",
    "            |  1   |   0\n",
    "        --------------\n",
    "Actual 1  |  80  |  20   â†’ TP=80, FN=20\n",
    "Actual 0  |  10  |  90   â†’ FP=10, TN=90\n",
    "```\n",
    "\n",
    "#### **Precision:**\n",
    "\\[\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{80}{80 + 10} = 0.89\n",
    "\\]\n",
    "\n",
    "#### **Recall:**\n",
    "\\[\n",
    "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{80}{80 + 20} = 0.80\n",
    "\\]\n",
    "\n",
    "#### **F1 Score:**\n",
    "\\[\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.89 \\cdot 0.80}{0.89 + 0.80} \\approx 0.84\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. Choosing the Right Evaluation Metric**\n",
    "\n",
    "Choosing the right metric depends on your business goal and the **cost of different errors**:\n",
    "\n",
    "- **Accuracy** is good for balanced data.\n",
    "- **Precision** when **false positives are costly**.\n",
    "- **Recall** when **false negatives are costly**.\n",
    "- **F1 Score** when you need a **balance between precision and recall**.\n",
    "- Use **ROC-AUC** for overall performance across thresholds.\n",
    "\n",
    "âœ”ï¸ **Tip:** Always check **class imbalance** first!\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. Example Where Precision is Most Important**\n",
    "\n",
    "**Problem:** Email Spam Classifier\n",
    "\n",
    "- Why precision?\n",
    "  - If you **misclassify important emails as spam** (false positives), users may miss critical information.\n",
    "  - Itâ€™s better to be conservative about marking something as spam.\n",
    "\n",
    "ðŸŽ¯ **Goal:** Maximize precision to avoid falsely labeling good emails as spam.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q9. Example Where Recall is Most Important**\n",
    "\n",
    "**Problem:** Cancer Detection (Medical Diagnosis)\n",
    "\n",
    "- Why recall?\n",
    "  - You don't want to **miss any real cases** of cancer (false negatives are dangerous).\n",
    "  - Itâ€™s okay to have some false alarms (false positives) if it means catching more real cases.\n",
    "\n",
    "ðŸŽ¯ **Goal:** Maximize recall to ensure **no positive cases are missed**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffe218",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
